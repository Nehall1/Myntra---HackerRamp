{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTRW1+EgugVeU+71b2BR+Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juhiibh/myntra-hackerramp/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIc5YLmNPs1I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input, VGG16\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "# Load the pre-trained VGG16 model\n",
        "model = VGG16(weights='imagenet', include_top=False)\n",
        "\n",
        "def extract_features(img_path, model): # The model is passed as an argument\n",
        "  try:\n",
        "    img = image.load_img(img_path,target_size=(224,224))\n",
        "    img_array = image.img_to_array(img) # Use image.img_to_array\n",
        "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "    preprocessed_img = preprocess_input(expanded_img_array)\n",
        "    result = model.predict(preprocessed_img).flatten()\n",
        "    normalized_result = result / norm(result)\n",
        "    return normalized_result\n",
        "  except Exception as e:\n",
        "    print(f\"Error processing image {img_path}: {e}\")\n",
        "    return None  # Return None for images that can't be processed\n",
        "\n",
        "filenames = []\n",
        "\n",
        "for file in os.listdir('/content/drive/MyDrive/cottage core pics'):\n",
        "    # Check if the file is a regular file and not a directory or hidden file\n",
        "    full_path = os.path.join('/content/drive/MyDrive/cottage core pics', file)\n",
        "    if os.path.isfile(full_path) and not file.startswith('.'):\n",
        "        filenames.append(full_path)\n",
        "\n",
        "print(len(filenames))\n",
        "print(filenames[0:5])\n",
        "\n",
        "feature_list = []\n",
        "\n",
        "# 'model' is now defined and can be used in the loop\n",
        "for file in tqdm(filenames):\n",
        "    features = extract_features(file, model)\n",
        "    if features is not None:  # Append only if features were successfully extracted\n",
        "        feature_list.append(features)\n",
        "\n",
        "pickle.dump(feature_list,open('embeddings.pkl','wb'))\n",
        "pickle.dump(filenames,open('embeddings.pkl','wb'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import tensorflow\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input\n",
        "\n",
        "\n",
        "model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3)) # Make sure 'imagenet' is spelled correctly\n",
        "model.trainable = False\n",
        "\n",
        "model = Sequential([ # Use Sequential from tensorflow.keras\n",
        "    model,\n",
        "    GlobalMaxPooling2D()\n",
        "])\n",
        "\n",
        "\n",
        "feature_list = pickle.load(open('embeddings.pkl','rb'))\n",
        "filenames = pickle.load(open('filenames.pkl','rb'))\n",
        "\n",
        "img = image.load_img('/content/drive/MyDrive/Screenshot' (122).png,target_size=(224,224))\n",
        "img_array = image.img_to_array(img) # Use image.img_to_array\n",
        "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "    preprocessed_img = preprocess_input(expanded_img_array)\n",
        "    result = model.predict(preprocessed_img).flatten()\n",
        "    normalized_result = result / norm(result)\n",
        "    return normalized_result\n",
        ""
      ],
      "metadata": {
        "id": "E0n7yGt0QUQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Load the ResNet50 model without the top layers\n",
        "model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "model.trainable = False\n",
        "\n",
        "# Create a Sequential model and add the ResNet50 base model and a GlobalMaxPooling2D layer\n",
        "sequential_model = tf.keras.Sequential([\n",
        "    model,\n",
        "    GlobalMaxPooling2D()\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "# print(sequential_model.summary())\n",
        "\n",
        "def extract_features(img_path,model):\n",
        "  img = image.load_img(img_path,target_size=(224,224))\n",
        "  img_array = image.img_to_array(img)\n",
        "  expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "  preprocessed_img = preprocess_input(expanded_img_array)\n",
        "  result = model.predict(preprocessed_img).flatten()\n",
        "  normalized_result = result / norm(result)\n",
        "\n",
        "  return normalized_result\n",
        "\n",
        "# Initialize filenames list before appending\n",
        "filenames = []\n",
        "feature_list = [] # Initialize feature_list before appending\n",
        "\n",
        "# print(os.listdir('/content/drive/MyDrive/cottage core pics'))\n",
        "\n",
        "for file in os.listdir('/content/drive/MyDrive/cottage core pics'):\n",
        "    # Construct full file path\n",
        "    full_path = os.path.join('/content/drive/MyDrive/cottage core pics', file)\n",
        "\n",
        "    # Check if it's a valid image file\n",
        "    if os.path.isfile(full_path) and file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        filenames.append(full_path)\n",
        "\n",
        "# print(len(filenames))\n",
        "# print(filenames[0:5])\n",
        "\n",
        "for file in tqdm(filenames):\n",
        "    feature_list.append(extract_features(file, model)) # Pass the 'model' object to the function\n",
        "\n",
        "# print(np.array(feature_list).shape)\n",
        "\n",
        "pickle.dump(feature_list,open('embeddings.pkl','wb'))\n",
        "pickle.dump(filenames,open('filenames.pkl','wb'))\n"
      ],
      "metadata": {
        "id": "zmscl-SxQW1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import tensorflow\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import cv2\n",
        "\n",
        "feature_list = np.array(pickle.load(open('embeddings.pkl','rb')))\n",
        "filenames = pickle.load(open('filenames.pkl','rb'))\n",
        "\n",
        "model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "model.trainable = False\n",
        "\n",
        "sequential_model = tf.keras.Sequential([\n",
        "    model,\n",
        "    GlobalMaxPooling2D()\n",
        "])\n",
        "\n",
        "\n",
        "img = image.load_img('/content/drive/MyDrive/Screenshot (122).png',target_size=(224,224))\n",
        "img_array = image.img_to_array(img)\n",
        "expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "preprocessed_img = preprocess_input(expanded_img_array)\n",
        "result = model.predict(preprocessed_img).flatten()\n",
        "normalized_result = result / norm(result)\n",
        "\n",
        "neighbors = NearestNeighbors(n_neighbors = 5 ,algorithm = 'brute',metric = 'euclidean')\n",
        "neighbors.fit(feature_list)\n",
        "\n",
        "distances , indices = neighbors.kneighbors([normalized_result])\n",
        "\n",
        "print(indices)\n",
        "\n",
        "for file in indices[0]:\n",
        "    temp_img = cv2.imread(filenames[file])\n",
        "    cv2_imshow(temp_img)\n",
        "\n",
        "cv2.waitKey(0)\n"
      ],
      "metadata": {
        "id": "l89LiCDFQavv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input, VGG16\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "# Load the pre-trained VGG16 model\n",
        "model = VGG16(weights='imagenet', include_top=False)\n",
        "\n",
        "def extract_features(img_path, model): # The model is passed as an argument\n",
        "  try:\n",
        "    img = image.load_img(img_path,target_size=(224,224))\n",
        "    img_array = image.img_to_array(img) # Use image.img_to_array\n",
        "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "    preprocessed_img = preprocess_input(expanded_img_array)\n",
        "    result = model.predict(preprocessed_img).flatten()\n",
        "    normalized_result = result / norm(result)\n",
        "    return normalized_result\n",
        "  except Exception as e:\n",
        "    print(f\"Error processing image {img_path}: {e}\")\n",
        "    return None  # Return None for images that can't be processed\n",
        "\n",
        "filenames = []\n",
        "\n",
        "for file in os.listdir('/content/drive/MyDrive/cottage core pics'):\n",
        "    # Check if the file is a regular file and not a directory or hidden file\n",
        "    full_path = os.path.join('/content/drive/MyDrive/cottage core pics', file)\n",
        "    if os.path.isfile(full_path) and not file.startswith('.'):\n",
        "        filenames.append(full_path)\n",
        "\n",
        "print(len(filenames))\n",
        "print(filenames[0:5])\n",
        "\n",
        "feature_list = []\n",
        "\n",
        "# 'model' is now defined and can be used in the loop\n",
        "for file in tqdm(filenames):\n",
        "    features = extract_features(file, model)\n",
        "    if features is not None:  # Append only if features were successfully extracted\n",
        "        feature_list.append(features)\n",
        "\n",
        "pickle.dump(feature_list,open('embeddings.pkl','wb'))\n",
        "pickle.dump(filenames,open('embeddings.pkl','wb'))"
      ],
      "metadata": {
        "id": "3iW-TeDBWhZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import tensorflow\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input\n",
        "\n",
        "\n",
        "model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3)) # Make sure 'imagenet' is spelled correctly\n",
        "model.trainable = False\n",
        "\n",
        "model = Sequential([ # Use Sequential from tensorflow.keras\n",
        "    model,\n",
        "    GlobalMaxPooling2D()\n",
        "])\n",
        "\n",
        "\n",
        "feature_list = pickle.load(open('embeddings.pkl','rb'))\n",
        "filenames = pickle.load(open('filenames.pkl','rb'))\n",
        "\n",
        "img = image.load_img('/content/drive/MyDrive/Screenshot' (122).png,target_size=(224,224))\n",
        "img_array = image.img_to_array(img) # Use image.img_to_array\n",
        "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "    preprocessed_img = preprocess_input(expanded_img_array)\n",
        "    result = model.predict(preprocessed_img).flatten()\n",
        "    normalized_result = result / norm(result)\n",
        "    return normalized_result"
      ],
      "metadata": {
        "id": "1jS6fLfsWnyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Load the ResNet50 model without the top layers\n",
        "model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "model.trainable = False\n",
        "\n",
        "# Create a Sequential model and add the ResNet50 base model and a GlobalMaxPooling2D layer\n",
        "sequential_model = tf.keras.Sequential([\n",
        "    model,\n",
        "    GlobalMaxPooling2D()\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "# print(sequential_model.summary())\n",
        "\n",
        "def extract_features(img_path,model):\n",
        "  img = image.load_img(img_path,target_size=(224,224))\n",
        "  img_array = image.img_to_array(img)\n",
        "  expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "  preprocessed_img = preprocess_input(expanded_img_array)\n",
        "  result = model.predict(preprocessed_img).flatten()\n",
        "  normalized_result = result / norm(result)\n",
        "\n",
        "  return normalized_result\n",
        "\n",
        "# Initialize filenames list before appending\n",
        "filenames = []\n",
        "feature_list = [] # Initialize feature_list before appending\n",
        "\n",
        "# print(os.listdir('/content/drive/MyDrive/cottage core pics'))\n",
        "\n",
        "for file in os.listdir('/content/drive/MyDrive/cottage core pics'):\n",
        "    # Construct full file path\n",
        "    full_path = os.path.join('/content/drive/MyDrive/cottage core pics', file)\n",
        "\n",
        "    # Check if it's a valid image file\n",
        "    if os.path.isfile(full_path) and file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        filenames.append(full_path)\n",
        "\n",
        "# print(len(filenames))\n",
        "# print(filenames[0:5])\n",
        "\n",
        "for file in tqdm(filenames):\n",
        "    feature_list.append(extract_features(file, model)) # Pass the 'model' object to the function\n",
        "\n",
        "# print(np.array(feature_list).shape)\n",
        "\n",
        "pickle.dump(feature_list,open('embeddings.pkl','wb'))\n",
        "pickle.dump(filenames,open('filenames.pkl','wb'))"
      ],
      "metadata": {
        "id": "lvB7AMfWWoy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import tensorflow\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import cv2\n",
        "\n",
        "feature_list = np.array(pickle.load(open('embeddings.pkl','rb')))\n",
        "filenames = pickle.load(open('filenames.pkl','rb'))\n",
        "\n",
        "model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "model.trainable = False\n",
        "\n",
        "sequential_model = tf.keras.Sequential([\n",
        "    model,\n",
        "    GlobalMaxPooling2D()\n",
        "])\n",
        "\n",
        "\n",
        "img = image.load_img('/content/drive/MyDrive/Screenshot (122).png',target_size=(224,224))\n",
        "img_array = image.img_to_array(img)\n",
        "expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "preprocessed_img = preprocess_input(expanded_img_array)\n",
        "result = model.predict(preprocessed_img).flatten()\n",
        "normalized_result = result / norm(result)\n",
        "\n",
        "neighbors = NearestNeighbors(n_neighbors = 5 ,algorithm = 'brute',metric = 'euclidean')\n",
        "neighbors.fit(feature_list)\n",
        "\n",
        "distances , indices = neighbors.kneighbors([normalized_result])\n",
        "\n",
        "print(indices)\n",
        "\n",
        "for file in indices[0]:\n",
        "    temp_img = cv2.imread(filenames[file])\n",
        "    cv2_imshow(temp_img)\n",
        "\n",
        "cv2.waitKey(0)"
      ],
      "metadata": {
        "id": "W7kKEHAVWr8C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}